# AutoML-CodeGen Configuration File
# This file contains default settings for neural architecture search and code generation

# Neural Architecture Search Configuration
search:
  algorithm: evolutionary              # Search algorithm: evolutionary, darts, reinforcement, hybrid
  population_size: 50                 # Number of architectures in population
  num_generations: 20                 # Number of evolution generations
  mutation_rate: 0.1                  # Probability of mutation
  crossover_rate: 0.8                 # Probability of crossover
  elitism_ratio: 0.1                  # Fraction of population to preserve as elite
  tournament_size: 3                  # Tournament selection size
  early_stopping: true                # Enable early stopping
  patience: 5                         # Early stopping patience
  max_age: 10                         # Maximum age before forced replacement
  adaptive_parameters: true           # Adapt parameters during search

# Architecture Evaluation Configuration
evaluation:
  max_epochs: 50                      # Maximum training epochs per architecture
  batch_size: 128                     # Training batch size
  learning_rate: 0.001                # Learning rate
  optimizer: adam                     # Optimizer: adam, sgd, adamw
  scheduler: cosine                   # LR scheduler: cosine, step, exponential
  validation_split: 0.2               # Validation split ratio
  early_stopping: true                # Enable early stopping during training
  patience: 5                         # Early stopping patience
  min_epochs: 10                      # Minimum epochs before early stopping
  use_mixed_precision: true           # Use mixed precision training

# Code Generation Configuration
codegen:
  target_framework: pytorch           # Target framework: pytorch, tensorflow, onnx, tensorrt
  optimization_level: 2               # Optimization level: 0-3
  include_training: true              # Include training code
  include_inference: true             # Include inference code
  include_deployment: false           # Include deployment code
  code_style: black                   # Code style: black, autopep8
  optimizations:                      # List of optimizations to apply
    - quantization
    - fusion
  target_device: auto                 # Target device: cpu, gpu, mobile, edge, auto

# Hardware Configuration
hardware:
  target_device: gpu                  # Primary target device
  memory_limit: 8GB                   # Memory limit
  latency_target: 100.0               # Target latency in milliseconds
  energy_budget: 1.0                  # Energy budget in Watts
  batch_size_limit: 256               # Maximum batch size
  use_tensorrt: false                 # Use TensorRT optimization
  use_quantization: true              # Use quantization

# Logging and Monitoring Configuration
logging:
  level: INFO                         # Logging level: DEBUG, INFO, WARNING, ERROR
  save_checkpoints: true              # Save training checkpoints
  checkpoint_frequency: 5             # Checkpoint save frequency
  log_frequency: 10                   # Log frequency (iterations)
  use_wandb: false                    # Use Weights & Biases
  use_tensorboard: true               # Use TensorBoard
  wandb_project: null                 # W&B project name
  wandb_entity: null                  # W&B entity name

# Dataset Configuration
data:
  dataset_name: cifar10               # Dataset: cifar10, cifar100, imagenet, custom
  data_path: null                     # Custom dataset path
  download: true                      # Auto-download dataset
  num_workers: 4                      # DataLoader workers
  pin_memory: true                    # Pin memory for faster transfer
  augmentation: true                  # Use data augmentation
  normalization: true                 # Normalize data
  cache_dataset: false                # Cache dataset in memory

# Experiment Configurations for Different Use Cases
experiments:
  
  # Quick development experiment
  dev:
    search:
      population_size: 10
      num_generations: 5
    evaluation:
      max_epochs: 10
      batch_size: 64
    logging:
      checkpoint_frequency: 2
  
  # Mobile-optimized search
  mobile:
    hardware:
      target_device: mobile
      memory_limit: 2GB
      latency_target: 50.0
      energy_budget: 0.5
    codegen:
      target_framework: tensorflow
      optimizations:
        - quantization
        - pruning
        - fusion
    search:
      population_size: 30
      num_generations: 15
  
  # High-performance search
  performance:
    search:
      population_size: 100
      num_generations: 50
      adaptive_parameters: true
    evaluation:
      max_epochs: 100
      batch_size: 256
      use_mixed_precision: true
    hardware:
      target_device: gpu
      memory_limit: 32GB
    logging:
      use_wandb: true
      wandb_project: automl-performance
  
  # Edge deployment experiment
  edge:
    hardware:
      target_device: edge
      memory_limit: 1GB
      latency_target: 25.0
      energy_budget: 0.3
    codegen:
      target_framework: onnx
      optimization_level: 3
      optimizations:
        - quantization
        - pruning
        - fusion
        - compilation
    search:
      objectives:
        - accuracy
        - latency
        - memory
        - energy
  
  # Research experiment with comprehensive search
  research:
    search:
      algorithm: hybrid
      population_size: 200
      num_generations: 100
      adaptive_parameters: true
    evaluation:
      max_epochs: 200
      early_stopping: false
    logging:
      use_wandb: true
      use_tensorboard: true
      log_frequency: 5
    codegen:
      include_deployment: true
      optimization_level: 3

# Advanced Search Space Configuration
search_space:
  # Layer types to include in search
  layer_types:
    - conv2d
    - depthwise_conv
    - pointwise_conv
    - linear
    - batchnorm
    - layernorm
    - relu
    - gelu
    - swish
    - attention
    - adaptive_pool
    - dropout
  
  # Architecture constraints
  constraints:
    min_layers: 3
    max_layers: 50
    min_channels: 16
    max_channels: 1024
    max_parameters: 50000000  # 50M parameters
    max_flops: 1000000000     # 1B FLOPs
  
  # Connection patterns
  connections:
    - sequential
    - residual
    - dense
    - attention

# Multi-objective optimization weights
objectives:
  weights:
    accuracy: 0.5
    latency: 0.25
    memory: 0.15
    energy: 0.1
  
  # Objective-specific configurations
  accuracy:
    metric: top1
    threshold: 0.9
  
  latency:
    measurement: inference_time
    device: target_device
    batch_size: 1
  
  memory:
    include_activations: true
    peak_memory: true
  
  energy:
    measurement_method: model_based
    include_data_transfer: true

# Distributed training configuration
distributed:
  backend: nccl                       # Communication backend
  init_method: env://                 # Initialization method
  world_size: 1                       # Number of processes
  rank: 0                             # Process rank
  
  # Multi-node settings
  master_addr: localhost
  master_port: 29500
  
  # Performance tuning
  bucket_cap_mb: 25
  find_unused_parameters: false
  gradient_as_bucket_view: true

# Visualization settings
visualization:
  dpi: 300                           # Figure DPI
  figsize: [12, 8]                   # Figure size
  style: seaborn                     # Plot style
  save_format: png                   # Save format
  
  # Plot types to generate
  plots:
    - convergence
    - pareto_front
    - search_progress
    - architecture_diagram
    - training_curves
    - performance_comparison

# Benchmarking configuration
benchmarking:
  num_runs: 100                      # Number of benchmark runs
  warmup_runs: 10                    # Warmup runs
  measure_memory: true               # Measure memory usage
  measure_energy: false              # Measure energy consumption
  profile_operations: false          # Profile individual operations
  
  # Hardware-specific benchmarking
  devices:
    - cpu
    - gpu
  
  batch_sizes:
    - 1
    - 16
    - 32
    - 64

# Environment and dependencies
environment:
  python_version: ">=3.8"
  cuda_version: ">=11.0"
  torch_version: ">=2.0.0"
  
  # Optional dependencies
  optional_deps:
    tensorrt: false
    onnx: true
    wandb: false
    flash_attention: false 